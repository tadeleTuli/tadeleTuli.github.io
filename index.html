<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Tadele B. Tuli


</title>
<meta name="description" content="Toward human centered Human-Robot collaboration! 
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>tuli.ico</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="tadeletuli.com/">
       <span class="font-weight-bold">Tadele</span> B.  Tuli
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/">
                
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/cv/">
                Curriculum vitae
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Tadele</span> B. Tuli
    </h1>
     <p class="desc"></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        


<img class="img-fluid z-depth-2 rounded-cicle" src="/assets/resized/prof_pic-1400x1863.jpeg" srcset="    /assets/resized/prof_pic-480x639.jpeg 480w,    /assets/resized/prof_pic-800x1065.jpeg 800w,    /assets/resized/prof_pic-1400x1863.jpeg 1400w,/assets/img/prof_pic.jpeg 1405w">

      
      
        <div class="address">
          <p> Tel: +49 271 8092 3636</p> <p>Siegen, Germany</p>

        </div>
      
    </div>
    

    <div class="clearfix">
      <ul>
  <li>
    <p>A researcher in the field of human-robot collaboration for production systems.</p>
  </li>
  <li>
    <p>Involved in additive manufacturing techniques as a side project, mainly for product customization and handling strategy.</p>
  </li>
  <li>
    <p>Deliver lecture and tutorial courses for B.Sc. and M.Sc. programs including Industrial Robotics, and Automation Technology at the <a href="https://protech.mb.uni-siegen.de/fams/" target="_blank" rel="noopener noreferrer">Universität Siegen, Germany</a>.</p>
  </li>
  <li>
    <p>Hold B.Sc. in Mechanical Engineering, M.Sc. in Manufacturing Engineering and M.Sc. in Mechatronics Engineering (Robotics curriculum) in 2008, 2012, and 2015, respectively.</p>
  </li>
  <li>
    <p>Former lecturer and associate dean for academics at the <a href="http://www.aastu.edu.et/academics/colleges/college-of-electrical-mechanical-engineering/" target="_blank" rel="noopener noreferrer">College of Electrical and Mechanical Engineering</a> in Addis Ababa Science and Technology University.</p>
  </li>
  <li>
    <p>For extended details, please check the author’s cv <a href="../assets/pdf/TULI_CV.pdf">here</a>.</p>
  </li>
</ul>


    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Jan 2, 2024</th>
          <td>
            
              <h1 id="2023-in-short">2023 in short</h1>

<ul>
  <li>
<strong>Journal article</strong>  Our article entitled as “Explainable human activity recognition based on probabilistic spatial partitions for 
symbiotic workplaces” was published on the ‘International Journal of Computer Integrated Manufacturing’ (Impact factor 4.1 (2022), 23% 
acceptance rate). Link <a href="https://doi.org/10.1080/0951192X.2023.2177742" target="_blank" rel="noopener noreferrer">here</a>.</li>
</ul>

<p align="center">
  <img width="480" height="220" src="../assets/img/haropp.jpg">


</p>
<ul>
  <li>
    <strong>Conference article</strong> Our article entitled as “A Motion Capture-Based Approach to Human Work Analysis for Industrial Assembly Workstations” was presented at CARV/MCPC 2023 at University of Bologna, Italy and the paper was recognized for the Best Young Fellow Paper Award.
  </li>
  <li>
    <strong>Tadele Belay Tuli</strong> was elected to the grade of Senior Member level in the IEEE and participates in <a href="https://tcfa.ieee-ies.org/members.html" target="_blank" rel="noopener noreferrer">Factory automation technical committee</a> and <a href="https://www.ieee-ras.org/" target="_blank" rel="noopener noreferrer">Robotics</a>.
  </li>
</ul>

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Aug 4, 2022</th>
          <td>
            
              <strong>Journal article</strong> : Our article that address human motion quality and accuracy issue for physical interaction of human and robot in collaborative tasks has been published open access on <em>Intelligent Service Robotics</em> journal. Your feedback and comment is very welcome. Please find it <a href="https://www.doi.org/10.1007/s11370-022-00432-8" target="_blank" rel="noopener noreferrer">here</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Apr 6, 2022</th>
          <td>
            
              Two papers have been accepted for presentation and publication for the summer <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">. The <a href="/https://cpsl-conference.com/">Conference on Production Systems and Logistics (CPSL)</a> will be held in Vancouver, Canada and The second conference is <a href="https://www.cirp-cms2022.org/" target="_blank" rel="noopener noreferrer">CMS2022</a> which is the 55th edition of the CIRP International Conference on Manufacturing Systems.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Nov 2, 2021</th>
          <td>
            
              CARV21/MCPC21-Best paper award 2021 <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Aug 6, 2021</th>
          <td>
            
              Stiphend for summer school in “Science communication” <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> .

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography">
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IJCIM</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/researching.svg">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_explainable_2023" class="col-sm-8">
    
      <div class="title">Explainable Human Activity Recognition Based on Probabilistic Spatial Partitions for Symbiotic Workplaces</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Journal of Computer Integrated Manufacturing</em>
      
      
        Feb
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1080/0951192X.2023.2177742" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>In recent years, smart workplaces that adapt to human activity and motion behavior have been proposed for cognitive production systems. In this respect, methods for identifying the feelings and activities of human workers are being investigated to improve the cognitive capability of smart machines such as robots in shared working spaces. Recognizing human activities and predicting the possible next sequence of operations may simplify robot programming and improve collaboration efficiency. However, human activity recognition still requires explainable models that are versatile, robust, and interpretative. Therefore, recognizing and analyzing human action details using continuous probability density estimates in different workplace layouts is essential. Three scenarios are considered: a standalone, a one-piece flow U-form, and a human-robot hybrid workplace. This work presents a novel approach to human activity recognition based on a probabilistic spatial partition (HAROPP). Its performance is compared to the geometric-bounded activity recognition method. Results show that spatial partitions based on probabilistic density contain 20% fewer data frames and 10% more spatial areas than the geometric bounding box. The approach, on average, detects human activities correctly for 81% of the cases for a pre-known workplace layout. HAROPP has scalability and applicability potential for cognitive workplaces with a digital twin in the loop for pushing the cognitive capabilities of machine systems and realizing human-centered environments.</p-->
	  <p style="color:Gray;">In recent years, smart workplaces that adapt to human activity and motion behavior have been proposed for cognitive production systems. In this respect, methods for identifying the feelings and activities of human workers are being investigated to improve the cognitive capability of smart machines such as robots in shared working spaces. Recognizing human activities and predicting the possible next sequence of operations may simplify robot programming and improve collaboration efficiency. However, human activity recognition still requires explainable models that are versatile, robust, and interpretative. Therefore, recognizing and analyzing human action details using continuous probability density estimates in different workplace layouts is essential. Three scenarios are considered: a standalone, a one-piece flow U-form, and a human-robot hybrid workplace. This work presents a novel approach to human activity recognition based on a probabilistic spatial partition (HAROPP). Its performance is compared to the geometric-bounded activity recognition method. Results show that spatial partitions based on probabilistic density contain 20% fewer data frames and 10% more spatial areas than the geometric bounding box. The approach, on average, detects human activities correctly for 81% of the cases for a pre-known workplace layout. HAROPP has scalability and applicability potential for cognitive workplaces with a digital twin in the loop for pushing the cognitive capabilities of machine systems and realizing human-centered environments.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_explainable_2023</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IJCIM}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Explainable Human Activity Recognition Based on Probabilistic Spatial Partitions for Symbiotic Workplaces}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Manns, Martin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Computer Integrated Manufacturing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{0}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{0}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--18}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{Taylor \&amp; Francis}}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0951-192X}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1080/0951192X.2023.2177742}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2023-03-27}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{continuous probabilistic model,Human activity recognition,human motion capture,human-robot collaboration,workplace layout}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge"><a href="https://www.springer.com/journal/12369" target="_blank" rel="noopener noreferrer">SORO</a></abbr>
	<!-- img class="col bibone first" src="tadeletuli.com/assets/img/motion_interaction.png"-->
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/motion_interaction.png">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_telepresence_2020" class="col-sm-8">
    
      <div class="title">Telepresence Mobile Robots Design and Control for Social Interaction</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Terefe, Tesfaye Olana,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Rashid, Md Mamun Ur
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Journal of Social Robotics</em>
      
      
        Jul
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/s12369-020-00676-3" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Human–robot interaction has extended its application horizon to simplify how human beings interact with each other through a remotely controlled telepresence robot. The fast growth of communication technologies such as 4G and 5G has elevated the potential to establish stable audio–video-data transmission. However, human–robot physical interactions are still challenging regarding maneuverability, controllability, stability, drive layout, and autonomy. Hence, this paper presents a systematic design and control approach based on the customer’s needs and expectations of telepresence mobile robots for social interactions. A system model and controller design are developed using the Lagrangian method and linear quadratic regulator, respectively, for different scenarios such as flat surface, inclined surface, and yaw (steering). The robot system is capable of traveling uphill (30\{^{}circ }\\∘) and has a variable height (600–1200 mm). The robot is advantageous in developing countries to fill the skill gaps as well as for sharing knowledge and expertise using a virtual and mobile physical presence.</p-->
	  <p style="color:Gray;">Human–robot interaction has extended its application horizon to simplify how human beings interact with each other through a remotely controlled telepresence robot. The fast growth of communication technologies such as 4G and 5G has elevated the potential to establish stable audio–video-data transmission. However, human–robot physical interactions are still challenging regarding maneuverability, controllability, stability, drive layout, and autonomy. Hence, this paper presents a systematic design and control approach based on the customer’s needs and expectations of telepresence mobile robots for social interactions. A system model and controller design are developed using the Lagrangian method and linear quadratic regulator, respectively, for different scenarios such as flat surface, inclined surface, and yaw (steering). The robot system is capable of traveling uphill (30\{^{}circ }\\∘) and has a variable height (600–1200 mm). The robot is advantageous in developing countries to fill the skill gaps as well as for sharing knowledge and expertise using a virtual and mobile physical presence.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_telepresence_2020</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{SORO}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Telepresence {Mobile} {Robots} {Design} and {Control} for {Social} {Interaction}}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1875-4805}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/s12369-020-00676-3}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s12369-020-00676-3}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2020-09-14}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Social Robotics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Terefe, Tesfaye Olana and Rashid, Md Mamun Ur}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CIRP CMS</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/dmu.gif">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="manns_identifying_2021" class="col-sm-8">
    
      <div class="title">Identifying human intention during assembly operations using wearable motion capturing systems including eye focus</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Schreiber, Florian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Procedia CIRP</em>
      
      
        Jan
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1016/j.procir.2021.11.155" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Simulating human motion behavior in assembly operations helps to create efficient collaboration plans for humans and robots. However, identifying human intention may require high quality human motion capture data in order to discriminate micro-actions and human attention. In this regard, a human motion capture setup that combines various systems such as joint body, finger, and eye trackers is proposed in combination with a methodology of identifying the intention of human operators as well as for predicting sequences of activities. The approach may lead to safer human-robot collaboration.</p-->
	  <p style="color:Gray;">Simulating human motion behavior in assembly operations helps to create efficient collaboration plans for humans and robots. However, identifying human intention may require high quality human motion capture data in order to discriminate micro-actions and human attention. In this regard, a human motion capture setup that combines various systems such as joint body, finger, and eye trackers is proposed in combination with a methodology of identifying the intention of human operators as well as for predicting sequences of activities. The approach may lead to safer human-robot collaboration.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">manns_identifying_2021</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{54th {CIRP} {CMS} 2021 - {Towards} {Digitalized} {Manufacturing} 4.0}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Identifying human intention during assembly operations using wearable motion capturing systems including eye focus}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{104}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2212-8271}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S2212827121010532}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.procir.2021.11.155}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2021-11-29}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Procedia CIRP}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Manns, Martin and Tuli, Tadele Belay and Schreiber, Florian}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{human-robot collaboration, Operator 4.0, Eye tracker, human motion capture}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{924--929}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CIRP CMS}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IEEE ETFA</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_knowledge-based_2021" class="col-sm-8">
    
      <div class="title">Knowledge-Based Digital Twin for Predicting Interactions in Human-Robot Collaboration</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kohl, Linus,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Chala, Sisay Adugna,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Ansari, Fazel
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )</em>
      
      
        Sep
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1109/ETFA45728.2021.9613342" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Semantic representation of motions in a human-robot collaborative environment is essential for agile design and development of digital twins (DT) towards ensuring efficient collaboration between humans and robots in hybrid work systems, e.g., in assembly operations. Dividing activities into actions helps to further conceptualize motion models for predicting what a human intends to do in a hybrid work system. However, it is not straightforward to identify human intentions in collaborative operations for robots to understand and collaborate. This paper presents a concept for semantic representation of human actions and intention prediction using a flexible task ontology interface in the semantic data hub stored in a domain knowledge base. This semantic data hub enables the construction of a DT with corresponding reasoning and simulation algorithms. Furthermore, a knowledge-based DT concept is used to analyze and verify the presented use-case of Human-Robot Collaboration in assembly operations. The preliminary evaluation showed a promising reduction of time for assembly tasks, which identifies the potential to i) improve efficiency reflected by reducing costs and errors and ultimately ii) assist human workers in improving decision making. Thus the contribution of the current work involves a marriage of machine learning, robotics, and ontology engineering into DT to improve human-robot interaction and productivity in a collaborative production environment.</p-->
	  <p style="color:Gray;">Semantic representation of motions in a human-robot collaborative environment is essential for agile design and development of digital twins (DT) towards ensuring efficient collaboration between humans and robots in hybrid work systems, e.g., in assembly operations. Dividing activities into actions helps to further conceptualize motion models for predicting what a human intends to do in a hybrid work system. However, it is not straightforward to identify human intentions in collaborative operations for robots to understand and collaborate. This paper presents a concept for semantic representation of human actions and intention prediction using a flexible task ontology interface in the semantic data hub stored in a domain knowledge base. This semantic data hub enables the construction of a DT with corresponding reasoning and simulation algorithms. Furthermore, a knowledge-based DT concept is used to analyze and verify the presented use-case of Human-Robot Collaboration in assembly operations. The preliminary evaluation showed a promising reduction of time for assembly tasks, which identifies the potential to i) improve efficiency reflected by reducing costs and errors and ultimately ii) assist human workers in improving decision making. Thus the contribution of the current work involves a marriage of machine learning, robotics, and ontology engineering into DT to improve human-robot interaction and productivity in a collaborative production environment.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tuli_knowledge-based_2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Knowledge-{Based} {Digital} {Twin} for {Predicting} {Interactions} in {Human}-{Robot} {Collaboration}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ETFA45728.2021.9613342}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 26th {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA} )}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Kohl, Linus and Chala, Sisay Adugna and Manns, Martin and Ansari, Fazel}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Collaboration, digital twin, Digital twin, human action models, human-robot interaction, Knowledge based systems, machine learning, Ontologies, ontology, Predictive models, Productivity, Semantics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--8}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IEEE ETFA}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{2021130209.pdf}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge"><a href="https://carv2020.com/" target="_blank" rel="noopener noreferrer">CIRP CARV</a></abbr>
	<!-- img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif"-->
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif">
    </div>
  </div> -->

  
  
    
    <abbr class="badge">Best paper award</abbr>
    
  
  </div>
  
  
  <div id="tuli_understanding_2022" class="col-sm-8">
    
      <div class="title">Understanding Shared Autonomy of Collaborative Humans Using Motion Capture System for Simulating Team Assembly</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Jonek, Michael
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Towards Sustainable Customization: Bridging Smart Products and Manufacturing Systems</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
      
      <a href="/assets/pdf/shared_autonomy_final.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/978-3-030-90700-6_59" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>In virtual production planning, simulating human motions helps to improve process planning and interaction efficiency. However, simulating multiple humans sharing tasks in a shared workplace requires understanding how human workers interact and share autonomy. In this regard, an Inertial Measurement Unit based motion capture is employed for understanding shifting roles and learning effects. Parameters such as total time, distance, and acceleration variances in repetition are considered for modeling collaborative motion interactions. The results distinguish motion patterns versus the undertaken interactions. This work may serve as an initial input to model interaction schemes and recognize human actions behavior during team assembly. Furthermore, the concept can be extended toward a human-robot shared autonomy.</p-->
	  <p style="color:Gray;">In virtual production planning, simulating human motions helps to improve process planning and interaction efficiency. However, simulating multiple humans sharing tasks in a shared workplace requires understanding how human workers interact and share autonomy. In this regard, an Inertial Measurement Unit based motion capture is employed for understanding shifting roles and learning effects. Parameters such as total time, distance, and acceleration variances in repetition are considered for modeling collaborative motion interactions. The results distinguish motion patterns versus the undertaken interactions. This work may serve as an initial input to model interaction schemes and recognize human actions behavior during team assembly. Furthermore, the concept can be extended toward a human-robot shared autonomy.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tuli_understanding_2022</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CIRP CARV}</span><span class="p">,</span>
  <span class="na">award</span> <span class="p">=</span> <span class="s">{Best paper award}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Lecture {Notes} in {Mechanical} {Engineering}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Understanding {Shared} {Autonomy} of {Collaborative} {Humans} {Using} {Motion} {Capture} {System} for {Simulating} {Team} {Assembly}}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-030-90700-6}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-030-90700-6_59}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Towards {Sustainable} {Customization}: {Bridging} {Smart} {Products} and {Manufacturing} {Systems}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer International Publishing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Manns, Martin and Jonek, Michael}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Andersen, Ann-Louise and Andersen, Rasmus and Brunoe, Thomas Ditlev and Larsen, Maria Stoettrup Schioenning and Nielsen, Kjeld and Napoleone, Alessia and Kjeldgaard, Stefan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Human motion capture, Shared autonomy, Team interaction, Manual assembly, Role shifting}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{527--534}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{shared_autonomy_final.pdf}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge"><a href="https://carv2020.com/" target="_blank" rel="noopener noreferrer">CIRP CARV</a></abbr>
	<!-- img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif"-->
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif">
    </div>
  </div> -->

  
  
    
    <abbr class="badge">Best Young Fellow Paper Award</abbr>
    
  
  </div>
  
  
  <div id="jonek_motion_2023" class="col-sm-8">
    
      <div class="title">A Motion Capture-Based Approach to Human Work Analysis for Industrial Assembly Workstations</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Jonek, Michael,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Production Processes and Product Evolution in the Age of Disruption</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/978-3-031-34821-1_59" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>In industry, manual work is becoming increasingly important despite high labor costs due to the trend towards smaller productionProduction volumes and a higher number of variants. In order to identify optimization potential and increase productivity, there is a strong need to analyze and understand manual processes. Especially in Small and Medium-sized Enterprises (SME), which have limited resources for classic process time analysis, these are rarely available. In this work, a method for automatic generation of process time analyses of manual assemblyManual assembly processes is presented. It employs an industrial human activity recognitionHuman activity recognition system. Human motion data is captured in the industrial environment for manual assemblyManual assembly operation. It is post processed using a spatial partitioning approach. The study shows about 76% of the manual operations in the proposed use case scenario are automatically detected and the remaining are hardly identified. It is shown that process time analysis can be carried out without expert knowledge and without significant manual effort and provides knowledge about the process, which can be used to identify optimization potentials to increase productivity.</p-->
	  <p style="color:Gray;">In industry, manual work is becoming increasingly important despite high labor costs due to the trend towards smaller productionProduction volumes and a higher number of variants. In order to identify optimization potential and increase productivity, there is a strong need to analyze and understand manual processes. Especially in Small and Medium-sized Enterprises (SME), which have limited resources for classic process time analysis, these are rarely available. In this work, a method for automatic generation of process time analyses of manual assemblyManual assembly processes is presented. It employs an industrial human activity recognitionHuman activity recognition system. Human motion data is captured in the industrial environment for manual assemblyManual assembly operation. It is post processed using a spatial partitioning approach. The study shows about 76% of the manual operations in the proposed use case scenario are automatically detected and the remaining are hardly identified. It is shown that process time analysis can be carried out without expert knowledge and without significant manual effort and provides knowledge about the process, which can be used to identify optimization potentials to increase productivity.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CIRP CMS</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/dmu.gif">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_latent_2022" class="col-sm-8">
    
      <div class="title">Latent Space Based Collaborative Motion Modeling from Motion Capture Data for Human Robot Collaboration</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Henkel, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Procedia CIRP</em>
      
      
        Jan
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1016/j.procir.2022.05.128" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Collaborative assembly operation is one of the current challenges regarding human robot collaboration (HRC). In this context, it is still unclear how robots and humans should behave in handling an object and anticipating mutual care. In many cases, the modeling of collaborative behaviors shows difficulties, which can be addressesd by simplifying motion modeling techniques. In the current work, we propose a latent space approach that combines functional principal component analysis to derive low dimensional features with Gaussian mixture models to generate high-likelihood motion behavior estimates. This approach may increase agility in task planning and reduce programming difficulties in HRC.</p-->
	  <p style="color:Gray;">Collaborative assembly operation is one of the current challenges regarding human robot collaboration (HRC). In this context, it is still unclear how robots and humans should behave in handling an object and anticipating mutual care. In many cases, the modeling of collaborative behaviors shows difficulties, which can be addressesd by simplifying motion modeling techniques. In the current work, we propose a latent space approach that combines functional principal component analysis to derive low dimensional features with Gaussian mixture models to generate high-likelihood motion behavior estimates. This approach may increase agility in task planning and reduce programming difficulties in HRC.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_latent_2022</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CIRP CMS}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Leading manufacturing systems transformation – {Proceedings} of the 55th {CIRP} {Conference} on {Manufacturing} {Systems} 2022}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Latent {Space} {Based} {Collaborative} {Motion} {Modeling} from {Motion} {Capture} {Data} for {Human} {Robot} {Collaboration}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{107}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2212-8271}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S2212827122004127}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.procir.2022.05.128}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-05-27}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Procedia CIRP}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Henkel, Martin and Manns, Martin}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{motion capture, principal component analysis, human robot collaboration, Gaussian mixture models, latent space, motion modeling}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1180--1185}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">JIST</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_human_2022" class="col-sm-8">
    
      <div class="title">Human motion quality and accuracy measuring method for human–robot physical interactions</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zeller, Sebastian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Intelligent Service Robotics</em>
      
      
        Jul
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/s11370-022-00432-8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>In human–robot collaboration (HRC), human motion capture can be considered an enabler for switching autonomy between humans and robots to create efﬁcient and safe operations. For this purpose, wearable motion tracking systems such as IMU and lighthouse-based systems have been used to transfer human joint motions into robot controller models. Due to reasons such as global positioning, drift, and occlusion, in some situations, e.g., HRC, both systems have been combined. However, it is still not clear if the motion quality (e.g., smoothness, naturalness, and spatial accuracy) is sufﬁcient when the human operator is in the loop. This article presents a novel approach for measuring human motion quality and accuracy in HRC. The human motion capture has been implemented in a laboratory environment with a repetition of forty-cycle operations. Human motion, speciﬁcally of the wrist, is guided by the robot tool center point (TCP), which is predeﬁned for generating circular and square motions. Compared to the robot TCP motion considered baseline, the hand wrist motion deviates up to 3 cm. The approach is valuable for understanding the quality of human motion behaviors and can be scaled up for various applications involving human and robot shared workplaces.</p-->
	  <p style="color:Gray;">In human–robot collaboration (HRC), human motion capture can be considered an enabler for switching autonomy between humans and robots to create efﬁcient and safe operations. For this purpose, wearable motion tracking systems such as IMU and lighthouse-based systems have been used to transfer human joint motions into robot controller models. Due to reasons such as global positioning, drift, and occlusion, in some situations, e.g., HRC, both systems have been combined. However, it is still not clear if the motion quality (e.g., smoothness, naturalness, and spatial accuracy) is sufﬁcient when the human operator is in the loop. This article presents a novel approach for measuring human motion quality and accuracy in HRC. The human motion capture has been implemented in a laboratory environment with a repetition of forty-cycle operations. Human motion, speciﬁcally of the wrist, is guided by the robot tool center point (TCP), which is predeﬁned for generating circular and square motions. Compared to the robot TCP motion considered baseline, the hand wrist motion deviates up to 3 cm. The approach is valuable for understanding the quality of human motion behaviors and can be scaled up for various applications involving human and robot shared workplaces.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_human_2022</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{JIST}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Human motion quality and accuracy measuring method for human–robot physical interactions}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1861-2776, 1861-2784}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://link.springer.com/10.1007/s11370-022-00432-8}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s11370-022-00432-8}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-08-04}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Intelligent Service Robotics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Manns, Martin and Zeller, Sebastian}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%74%61%64%65%6D%65%63%68%32%30%30%38@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>
<a href="https://orcid.org/0000-0002-6769-0646" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a>
<a href="https://scholar.google.com/citations?user=1E7yz60AAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
<a href="https://publons.com/a/AAW-2703-2020/" title="Publons" target="_blank" rel="noopener noreferrer"><i class="ai ai-publons"></i></a>
<a href="https://www.researchgate.net/profile/Tadele-Tuli-2/" title="ResearchGate" target="_blank" rel="noopener noreferrer"><i class="ai ai-researchgate"></i></a>
<a href="https://github.com/tadeletuli" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/tadele-belay-tuli-76322258" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/tadeletuli" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>



<a href="https://protech.mb.uni-siegen.de/fams/team/tuli.html" title="Work" target="_blank" rel="noopener noreferrer"><i class="fas fa-briefcase"></i></a>









      </div>
      <div class="contact-note">Contact me.
</div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2024 Tadele B. Tuli.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
    Last updated: January 04, 2024.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
