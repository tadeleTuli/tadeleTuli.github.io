<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="tadeletuli.com/feed.xml" rel="self" type="application/atom+xml" /><link href="tadeletuli.com/" rel="alternate" type="text/html" /><updated>2024-01-04T19:21:06+01:00</updated><id>tadeletuli.com/feed.xml</id><title type="html">blank</title><subtitle>Toward human centered Human-Robot collaboration! 
</subtitle><entry><title type="html">HAROPP - Human action recognition based on probabilistic partitioning</title><link href="tadeletuli.com/Projects/2022/HAROPP/" rel="alternate" type="text/html" title="HAROPP - Human action recognition based on probabilistic partitioning" /><published>2022-07-31T23:00:00+02:00</published><updated>2022-07-31T23:00:00+02:00</updated><id>tadeletuli.com/Projects/2022/HAROPP</id><content type="html" xml:base="tadeletuli.com/Projects/2022/HAROPP/">&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;580&quot; height=&quot;260&quot; src=&quot;/assets/img/haropp.jpg&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In recent years, smart workplaces that adapt to human activity and motion behavior have been proposed for cognitive production systems. In this respect, methods for identifying the feelings and activities of human workers are being investigated to improve the cognitive capability of smart machines such as robots in shared working spaces. It is believed that, recognizing human activities and being able to predict the possible next sequence of operations may simplify robot programming as well as improve collaboration efficiency.
However, human activity recognition still requires explainable models that are versatile, robust, and interpretative. Hence, human action recognition based on continuous probability density estimates can be considered as the initial steps of the collaborative workplace design. Three scenarios are considered: a standalone, a one-piece flow U-form, and a human-robot hybrid workplace are evaluated and the results are published on &lt;a href=&quot;https://doi.org/10.1080/0951192X.2023.2177742&quot;&gt;international journal of computer integrated manufacturing (IJCIM)&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="Human action" /><category term="Human motion analysis" /><category term="Human robot collaboration" /><summary type="html"></summary></entry><entry><title type="html">HARNets - Human activity recognition networks using sequential memory networks</title><link href="tadeletuli.com/Projects/2022/HARNets/" rel="alternate" type="text/html" title="HARNets - Human activity recognition networks using sequential memory networks" /><published>2022-04-30T23:00:00+02:00</published><updated>2022-04-30T23:00:00+02:00</updated><id>tadeletuli.com/Projects/2022/HARNets</id><content type="html" xml:base="tadeletuli.com/Projects/2022/HARNets/">&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;480&quot; height=&quot;220&quot; src=&quot;/assets/img/HARNet.png&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Prediction of human activity and detection of subsequent actions is crucial for improving the interaction between humans and robots during collaborative operations. Deep-learning techniques are being applied to recognize human activities, including industrial applications. However, the lack of sufficient dataset in the industrial domain and complexities of some industrial activities such as screw driving, assembling small parts, and others affect the model development and testing of human activities. Recently, the InHard dataset (Industrial Human Activity Recognition Dataset) has been published to facilitate industrial human activity recognition for better human-robot collaboration, which still lacks extended evaluation. In this regard, we employ human activity recognition memory and sequential networks (HARNets) combining convolutional neural network (CNN) and long short-term memory (LSTM) techniques.&lt;/p&gt;

&lt;p&gt;Details are available &lt;a href=&quot;https://github.com/tadeleTuli/HARNets&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="Robotics" /><category term="human action recognition" /><category term="LSTM + CNN" /><category term="Human robot collaboration" /><summary type="html"></summary></entry><entry><title type="html">MoQua - Human motion quality analysis for human robot collaboration tasks</title><link href="tadeletuli.com/Projects/2021/MoQua/" rel="alternate" type="text/html" title="MoQua - Human motion quality analysis for human robot collaboration tasks" /><published>2021-04-30T23:00:00+02:00</published><updated>2021-04-30T23:00:00+02:00</updated><id>tadeletuli.com/Projects/2021/MoQua</id><content type="html" xml:base="tadeletuli.com/Projects/2021/MoQua/">&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;780&quot; height=&quot;260&quot; src=&quot;/assets/img/htc_motion.png&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In humanâ€“robot collaboration (HRC), human motion capture can be considered an enabler for switching autonomy between humans and robots to create efficient and safe operations. For this purpose, wearable motion tracking systems such as IMU and lighthouse-based systems have been used to transfer human joint motions into robot controller models. Due to reasons such as global positioning, drift, and occlusion, in some situations, e.g., HRC, both systems have been combined. However, it is still not clear if the motion quality (e.g., smoothness, naturalness, and spatial accuracy) is sufficient when the human operator is in the loop. This article presents a novel approach for measuring human motion quality and accuracy in HRC. The human motion capture has been implemented in a laboratory environment with a repetition of forty-cycle operations. Human motion, specifically of the wrist, is guided by the robot tool center point (TCP), which is predefined for generating circular and square motions. Compared to the robot TCP motion considered baseline, the hand wrist motion deviates up to 3 cm. The approach is valuable for understanding the quality of human motion behaviors and can be scaled up for various applications involving human and robot shared workplaces.&lt;/p&gt;

&lt;p&gt;The details of the work is published on the &lt;a href=&quot;https://doi.org/10.1007/s11370-022-00432-8&quot;&gt;international journal of Intelligent Service Robotics&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="Robotics" /><category term="Human motion quality" /><category term="Human motion capturing" /><category term="Human robot collaboration" /><summary type="html"></summary></entry></feed>