<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Tadele B. Tuli


  | Publications

</title>
<meta name="description" content="Toward human centered Human-Robot collaboration! 
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>tuli.ico</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publications/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="tadeletuli.com/">
       <span class="font-weight-bold">Tadele</span> B.  Tuli
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/">
                
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/cv/">
                Curriculum vitae
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description">Publications by categories in reversed chronological order generated by jekyll-scholar.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2024</h2>
  <ol class="bibliography"></ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IJCIM</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/researching.svg">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_explainable_2023" class="col-sm-8">
    
      <div class="title">Explainable Human Activity Recognition Based on Probabilistic Spatial Partitions for Symbiotic Workplaces</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Journal of Computer Integrated Manufacturing</em>
      
      
        Feb
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1080/0951192X.2023.2177742" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>In recent years, smart workplaces that adapt to human activity and motion behavior have been proposed for cognitive production systems. In this respect, methods for identifying the feelings and activities of human workers are being investigated to improve the cognitive capability of smart machines such as robots in shared working spaces. Recognizing human activities and predicting the possible next sequence of operations may simplify robot programming and improve collaboration efficiency. However, human activity recognition still requires explainable models that are versatile, robust, and interpretative. Therefore, recognizing and analyzing human action details using continuous probability density estimates in different workplace layouts is essential. Three scenarios are considered: a standalone, a one-piece flow U-form, and a human-robot hybrid workplace. This work presents a novel approach to human activity recognition based on a probabilistic spatial partition (HAROPP). Its performance is compared to the geometric-bounded activity recognition method. Results show that spatial partitions based on probabilistic density contain 20% fewer data frames and 10% more spatial areas than the geometric bounding box. The approach, on average, detects human activities correctly for 81% of the cases for a pre-known workplace layout. HAROPP has scalability and applicability potential for cognitive workplaces with a digital twin in the loop for pushing the cognitive capabilities of machine systems and realizing human-centered environments.</p-->
	  <p style="color:Gray;">In recent years, smart workplaces that adapt to human activity and motion behavior have been proposed for cognitive production systems. In this respect, methods for identifying the feelings and activities of human workers are being investigated to improve the cognitive capability of smart machines such as robots in shared working spaces. Recognizing human activities and predicting the possible next sequence of operations may simplify robot programming and improve collaboration efficiency. However, human activity recognition still requires explainable models that are versatile, robust, and interpretative. Therefore, recognizing and analyzing human action details using continuous probability density estimates in different workplace layouts is essential. Three scenarios are considered: a standalone, a one-piece flow U-form, and a human-robot hybrid workplace. This work presents a novel approach to human activity recognition based on a probabilistic spatial partition (HAROPP). Its performance is compared to the geometric-bounded activity recognition method. Results show that spatial partitions based on probabilistic density contain 20% fewer data frames and 10% more spatial areas than the geometric bounding box. The approach, on average, detects human activities correctly for 81% of the cases for a pre-known workplace layout. HAROPP has scalability and applicability potential for cognitive workplaces with a digital twin in the loop for pushing the cognitive capabilities of machine systems and realizing human-centered environments.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_explainable_2023</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IJCIM}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Explainable Human Activity Recognition Based on Probabilistic Spatial Partitions for Symbiotic Workplaces}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Manns, Martin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Computer Integrated Manufacturing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{0}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{0}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--18}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{Taylor \&amp; Francis}}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0951-192X}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1080/0951192X.2023.2177742}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2023-03-27}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{continuous probabilistic model,Human activity recognition,human motion capture,human-robot collaboration,workplace layout}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge"><a href="https://carv2020.com/" target="_blank" rel="noopener noreferrer">CIRP CARV</a></abbr>
	<!-- img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif"-->
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif">
    </div>
  </div> -->

  
  
    
    <abbr class="badge">Best Young Fellow Paper Award</abbr>
    
  
  </div>
  
  
  <div id="jonek_motion_2023" class="col-sm-8">
    
      <div class="title">A Motion Capture-Based Approach to Human Work Analysis for Industrial Assembly Workstations</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Jonek, Michael,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Production Processes and Product Evolution in the Age of Disruption</em>
      
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/978-3-031-34821-1_59" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>In industry, manual work is becoming increasingly important despite high labor costs due to the trend towards smaller productionProduction volumes and a higher number of variants. In order to identify optimization potential and increase productivity, there is a strong need to analyze and understand manual processes. Especially in Small and Medium-sized Enterprises (SME), which have limited resources for classic process time analysis, these are rarely available. In this work, a method for automatic generation of process time analyses of manual assemblyManual assembly processes is presented. It employs an industrial human activity recognitionHuman activity recognition system. Human motion data is captured in the industrial environment for manual assemblyManual assembly operation. It is post processed using a spatial partitioning approach. The study shows about 76% of the manual operations in the proposed use case scenario are automatically detected and the remaining are hardly identified. It is shown that process time analysis can be carried out without expert knowledge and without significant manual effort and provides knowledge about the process, which can be used to identify optimization potentials to increase productivity.</p-->
	  <p style="color:Gray;">In industry, manual work is becoming increasingly important despite high labor costs due to the trend towards smaller productionProduction volumes and a higher number of variants. In order to identify optimization potential and increase productivity, there is a strong need to analyze and understand manual processes. Especially in Small and Medium-sized Enterprises (SME), which have limited resources for classic process time analysis, these are rarely available. In this work, a method for automatic generation of process time analyses of manual assemblyManual assembly processes is presented. It employs an industrial human activity recognitionHuman activity recognition system. Human motion data is captured in the industrial environment for manual assemblyManual assembly operation. It is post processed using a spatial partitioning approach. The study shows about 76% of the manual operations in the proposed use case scenario are automatically detected and the remaining are hardly identified. It is shown that process time analysis can be carried out without expert knowledge and without significant manual effort and provides knowledge about the process, which can be used to identify optimization potentials to increase productivity.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge"><a href="https://carv2020.com/" target="_blank" rel="noopener noreferrer">CIRP CARV</a></abbr>
	<!-- img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif"-->
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_comparison_2022" class="col-sm-8">
    
      <div class="title">Comparison of AI-based Task Planning Approaches for Simulating Human-Robot Collaboration</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Towards Sustainable Customization: Bridging Smart Products and Manufacturing Systems</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/978-3-030-90700-6_17" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Today, increased demands for personalized products are making human-robot collaborative tasks a focus of research mainly for improving production cycle time, precision, and accuracy. It is also required to simplify how human-robot tasks and motions are generated. A graphical flow control-based programming can be one of such methods. This work investigates whether the graphical approaches (e.g., using RAFCON) yield a better real-time simulation or not compared to agent approaches (e.g., using MOSIM-AJAN). This work may support the agility of the digital manufacturing process by enhancing the efficiency of human-robot collaboration.</p-->
	  <p style="color:Gray;">Today, increased demands for personalized products are making human-robot collaborative tasks a focus of research mainly for improving production cycle time, precision, and accuracy. It is also required to simplify how human-robot tasks and motions are generated. A graphical flow control-based programming can be one of such methods. This work investigates whether the graphical approaches (e.g., using RAFCON) yield a better real-time simulation or not compared to agent approaches (e.g., using MOSIM-AJAN). This work may support the agility of the digital manufacturing process by enhancing the efficiency of human-robot collaboration.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge"><a href="https://carv2020.com/" target="_blank" rel="noopener noreferrer">CIRP CARV</a></abbr>
	<!-- img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif"-->
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif">
    </div>
  </div> -->

  
  
    
    <abbr class="badge">Best paper award</abbr>
    
  
  </div>
  
  
  <div id="tuli_understanding_2022" class="col-sm-8">
    
      <div class="title">Understanding Shared Autonomy of Collaborative Humans Using Motion Capture System for Simulating Team Assembly</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Jonek, Michael
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Towards Sustainable Customization: Bridging Smart Products and Manufacturing Systems</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
      
      <a href="/assets/pdf/shared_autonomy_final.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
      
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/978-3-030-90700-6_59" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>In virtual production planning, simulating human motions helps to improve process planning and interaction efficiency. However, simulating multiple humans sharing tasks in a shared workplace requires understanding how human workers interact and share autonomy. In this regard, an Inertial Measurement Unit based motion capture is employed for understanding shifting roles and learning effects. Parameters such as total time, distance, and acceleration variances in repetition are considered for modeling collaborative motion interactions. The results distinguish motion patterns versus the undertaken interactions. This work may serve as an initial input to model interaction schemes and recognize human actions behavior during team assembly. Furthermore, the concept can be extended toward a human-robot shared autonomy.</p-->
	  <p style="color:Gray;">In virtual production planning, simulating human motions helps to improve process planning and interaction efficiency. However, simulating multiple humans sharing tasks in a shared workplace requires understanding how human workers interact and share autonomy. In this regard, an Inertial Measurement Unit based motion capture is employed for understanding shifting roles and learning effects. Parameters such as total time, distance, and acceleration variances in repetition are considered for modeling collaborative motion interactions. The results distinguish motion patterns versus the undertaken interactions. This work may serve as an initial input to model interaction schemes and recognize human actions behavior during team assembly. Furthermore, the concept can be extended toward a human-robot shared autonomy.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tuli_understanding_2022</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CIRP CARV}</span><span class="p">,</span>
  <span class="na">award</span> <span class="p">=</span> <span class="s">{Best paper award}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Lecture {Notes} in {Mechanical} {Engineering}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Understanding {Shared} {Autonomy} of {Collaborative} {Humans} {Using} {Motion} {Capture} {System} for {Simulating} {Team} {Assembly}}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-030-90700-6}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-030-90700-6_59}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Towards {Sustainable} {Customization}: {Bridging} {Smart} {Products} and {Manufacturing} {Systems}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer International Publishing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Manns, Martin and Jonek, Michael}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Andersen, Ann-Louise and Andersen, Rasmus and Brunoe, Thomas Ditlev and Larsen, Maria Stoettrup Schioenning and Nielsen, Kjeld and Napoleone, Alessia and Kjeldgaard, Stefan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Human motion capture, Shared autonomy, Team interaction, Manual assembly, Role shifting}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{527--534}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{shared_autonomy_final.pdf}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge"><a href="https://carv2020.com/" target="_blank" rel="noopener noreferrer">CIRP CARV</a></abbr>
	<!-- img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif"-->
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="jonek_constraints_2022" class="col-sm-8">
    
      <div class="title">Constraints for Motion Generation in Work Planning with Digital Human Simulations</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Jonek, Michael,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Towards Sustainable Customization: Bridging Smart Products and Manufacturing Systems</em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/978-3-030-90700-6_64" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Flexible and varied manual assembly processes in the automotive industry are often based on manual labor. While simulation can be used to improve planning to maximize efficiency while minimizing ergonomic issues for workers, common simulation tools require extensive modeling time. In such simulations, the users are often process engineers who want to easily create complex human motion simulations. This paper presents a concept developed to create complex human motions for interacting with objects in a production environment with little effort. The concept separates between geometric constraints and the semantic meaning of the respective geometry. With a set of data types developed for this purpose based on a unified ontology, a range of geometric and semantic information can be specified for arbitrary objects. In this way, an action-specific motion generator can be used to define the appropriate motion for the interaction with an object depending on the action without defining case-specific constraints. For a first proof, the concept is tested and demonstrated in the assembly of a pedal car and sitting on a chair at a manual workstation. Based on the use case, the effect of effort reduction is shown.</p-->
	  <p style="color:Gray;">Flexible and varied manual assembly processes in the automotive industry are often based on manual labor. While simulation can be used to improve planning to maximize efficiency while minimizing ergonomic issues for workers, common simulation tools require extensive modeling time. In such simulations, the users are often process engineers who want to easily create complex human motion simulations. This paper presents a concept developed to create complex human motions for interacting with objects in a production environment with little effort. The concept separates between geometric constraints and the semantic meaning of the respective geometry. With a set of data types developed for this purpose based on a unified ontology, a range of geometric and semantic information can be specified for arbitrary objects. In this way, an action-specific motion generator can be used to define the appropriate motion for the interaction with an object depending on the action without defining case-specific constraints. For a first proof, the concept is tested and demonstrated in the assembly of a pedal car and sitting on a chair at a manual workstation. Based on the use case, the effect of effort reduction is shown.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jonek_constraints_2022</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CIRP CARV}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Lecture {Notes} in {Mechanical} {Engineering}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Constraints for {Motion} {Generation} in {Work} {Planning} with {Digital} {Human} {Simulations}}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-030-90700-6}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-030-90700-6_64}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Towards {Sustainable} {Customization}: {Bridging} {Smart} {Products} and {Manufacturing} {Systems}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer International Publishing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jonek, Michael and Tuli, Tadele Belay and Manns, Martin}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Andersen, Ann-Louise and Andersen, Rasmus and Brunoe, Thomas Ditlev and Larsen, Maria Stoettrup Schioenning and Nielsen, Kjeld and Napoleone, Alessia and Kjeldgaard, Stefan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Human motion simulation, Manufacturing simulation, Geometric constraints, Manufacturing planning, MOSIM}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{567--574}</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{constraint_final.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CIRP CMS</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/dmu.gif">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_latent_2022" class="col-sm-8">
    
      <div class="title">Latent Space Based Collaborative Motion Modeling from Motion Capture Data for Human Robot Collaboration</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Henkel, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Procedia CIRP</em>
      
      
        Jan
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1016/j.procir.2022.05.128" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Collaborative assembly operation is one of the current challenges regarding human robot collaboration (HRC). In this context, it is still unclear how robots and humans should behave in handling an object and anticipating mutual care. In many cases, the modeling of collaborative behaviors shows difficulties, which can be addressesd by simplifying motion modeling techniques. In the current work, we propose a latent space approach that combines functional principal component analysis to derive low dimensional features with Gaussian mixture models to generate high-likelihood motion behavior estimates. This approach may increase agility in task planning and reduce programming difficulties in HRC.</p-->
	  <p style="color:Gray;">Collaborative assembly operation is one of the current challenges regarding human robot collaboration (HRC). In this context, it is still unclear how robots and humans should behave in handling an object and anticipating mutual care. In many cases, the modeling of collaborative behaviors shows difficulties, which can be addressesd by simplifying motion modeling techniques. In the current work, we propose a latent space approach that combines functional principal component analysis to derive low dimensional features with Gaussian mixture models to generate high-likelihood motion behavior estimates. This approach may increase agility in task planning and reduce programming difficulties in HRC.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_latent_2022</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CIRP CMS}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Leading manufacturing systems transformation – {Proceedings} of the 55th {CIRP} {Conference} on {Manufacturing} {Systems} 2022}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Latent {Space} {Based} {Collaborative} {Motion} {Modeling} from {Motion} {Capture} {Data} for {Human} {Robot} {Collaboration}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{107}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2212-8271}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S2212827122004127}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.procir.2022.05.128}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-05-27}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Procedia CIRP}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Henkel, Martin and Manns, Martin}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{motion capture, principal component analysis, human robot collaboration, Gaussian mixture models, latent space, motion modeling}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1180--1185}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CPSL</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_industrial_2022" class="col-sm-8">
    
      <div class="title">Industrial Human Activity Prediction and Detection Using Sequential Memory Networks</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Patel, Valay Mukesh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.15488/12144" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Prediction of human activity and detection of subsequent actions is crucial for improving the interaction between humans and robots during collaborative operations. Deep-learning techniques are being applied to recognize human activities, including industrial applications. However, the lack of sufficient dataset in the industrial domain and complexities of some industrial activities such as screw driving, assembling small parts, and others affect the model development and testing of human activities. The InHard dataset (Industrial Human Activity Recognition Dataset) was recently published to facilitate industrial human activity recognition for better human-robot collaboration, which still lacks extended evaluation. We propose an activity recognition method using a combined convolutional neural network (CNN) and long short-term memory (LSTM) techniques to evaluate the InHard dataset and compare it with a new dataset captured in a lab environment. This method improves the success rate of activity recognition by processing temporal and spatial information. Accordingly, the accuracy of the dataset is tested using labeled lists of activities from IMU and video data. A model is trained and tested for nine low-level activity classes with approximately 400 samples per class. The test result shows 88% accuracy for IMU-based skeleton data, 77% for RGB spatial video, and 63% for RGB video-based skeleton. The result has been verified using a previously published region-based activity recognition. The proposed approach can be extended to push the cognition capability of robots in human-centric workplaces.</p-->
	  <p style="color:Gray;">Prediction of human activity and detection of subsequent actions is crucial for improving the interaction between humans and robots during collaborative operations. Deep-learning techniques are being applied to recognize human activities, including industrial applications. However, the lack of sufficient dataset in the industrial domain and complexities of some industrial activities such as screw driving, assembling small parts, and others affect the model development and testing of human activities. The InHard dataset (Industrial Human Activity Recognition Dataset) was recently published to facilitate industrial human activity recognition for better human-robot collaboration, which still lacks extended evaluation. We propose an activity recognition method using a combined convolutional neural network (CNN) and long short-term memory (LSTM) techniques to evaluate the InHard dataset and compare it with a new dataset captured in a lab environment. This method improves the success rate of activity recognition by processing temporal and spatial information. Accordingly, the accuracy of the dataset is tested using labeled lists of activities from IMU and video data. A model is trained and tested for nine low-level activity classes with approximately 400 samples per class. The test result shows 88% accuracy for IMU-based skeleton data, 77% for RGB spatial video, and 63% for RGB video-based skeleton. The result has been verified using a previously published region-based activity recognition. The proposed approach can be extended to push the cognition capability of robots in human-centric workplaces.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Zenodo</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_harnets_2022" class="col-sm-8">
    
      <div class="title">HARNets: Human Activity Recognition Networks Based on Python Programming Language</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Patel, Valay Mukesh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
    
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.5281/zenodo.6366665" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">JIST</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_human_2022" class="col-sm-8">
    
      <div class="title">Human motion quality and accuracy measuring method for human–robot physical interactions</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zeller, Sebastian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Intelligent Service Robotics</em>
      
      
        Jul
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/s11370-022-00432-8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>In human–robot collaboration (HRC), human motion capture can be considered an enabler for switching autonomy between humans and robots to create efﬁcient and safe operations. For this purpose, wearable motion tracking systems such as IMU and lighthouse-based systems have been used to transfer human joint motions into robot controller models. Due to reasons such as global positioning, drift, and occlusion, in some situations, e.g., HRC, both systems have been combined. However, it is still not clear if the motion quality (e.g., smoothness, naturalness, and spatial accuracy) is sufﬁcient when the human operator is in the loop. This article presents a novel approach for measuring human motion quality and accuracy in HRC. The human motion capture has been implemented in a laboratory environment with a repetition of forty-cycle operations. Human motion, speciﬁcally of the wrist, is guided by the robot tool center point (TCP), which is predeﬁned for generating circular and square motions. Compared to the robot TCP motion considered baseline, the hand wrist motion deviates up to 3 cm. The approach is valuable for understanding the quality of human motion behaviors and can be scaled up for various applications involving human and robot shared workplaces.</p-->
	  <p style="color:Gray;">In human–robot collaboration (HRC), human motion capture can be considered an enabler for switching autonomy between humans and robots to create efﬁcient and safe operations. For this purpose, wearable motion tracking systems such as IMU and lighthouse-based systems have been used to transfer human joint motions into robot controller models. Due to reasons such as global positioning, drift, and occlusion, in some situations, e.g., HRC, both systems have been combined. However, it is still not clear if the motion quality (e.g., smoothness, naturalness, and spatial accuracy) is sufﬁcient when the human operator is in the loop. This article presents a novel approach for measuring human motion quality and accuracy in HRC. The human motion capture has been implemented in a laboratory environment with a repetition of forty-cycle operations. Human motion, speciﬁcally of the wrist, is guided by the robot tool center point (TCP), which is predeﬁned for generating circular and square motions. Compared to the robot TCP motion considered baseline, the hand wrist motion deviates up to 3 cm. The approach is valuable for understanding the quality of human motion behaviors and can be scaled up for various applications involving human and robot shared workplaces.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_human_2022</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{JIST}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Human motion quality and accuracy measuring method for human–robot physical interactions}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1861-2776, 1861-2784}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://link.springer.com/10.1007/s11370-022-00432-8}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s11370-022-00432-8}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-08-04}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Intelligent Service Robotics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Manns, Martin and Zeller, Sebastian}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Book Chapter</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="kumar2022" class="col-sm-8">
    
      <div class="title">Advances in Additive Manufacturing - 1st Edition</div>
      <div class="author">
        
          
          
          
          
          
          
            
              <em>Tuli, Tadele Belay</em>
            
          
        
      </div>

      <div class="periodical">
      
      
      
        2022
      
      </div>
    

    <div class="links">
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inbook</span><span class="p">{</span><span class="nl">kumar2022</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{Book Chapter}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">chapter</span> <span class="p">=</span> <span class="s">{Path planning and simulation of 3d printer machine tool for development of prototypes}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Ajay Kumar, Ravi Mittal, Abid Haleem}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Advances in {Additive} {Manufacturing} - 1st {Edition}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.elsevier.com/books/advances-in-additive-manufacturing/kumar/978-0-323-91834-3}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CIRP CMS</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/dmu.gif">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="manns_identifying_2021" class="col-sm-8">
    
      <div class="title">Identifying human intention during assembly operations using wearable motion capturing systems including eye focus</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Schreiber, Florian
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Procedia CIRP</em>
      
      
        Jan
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1016/j.procir.2021.11.155" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Simulating human motion behavior in assembly operations helps to create efficient collaboration plans for humans and robots. However, identifying human intention may require high quality human motion capture data in order to discriminate micro-actions and human attention. In this regard, a human motion capture setup that combines various systems such as joint body, finger, and eye trackers is proposed in combination with a methodology of identifying the intention of human operators as well as for predicting sequences of activities. The approach may lead to safer human-robot collaboration.</p-->
	  <p style="color:Gray;">Simulating human motion behavior in assembly operations helps to create efficient collaboration plans for humans and robots. However, identifying human intention may require high quality human motion capture data in order to discriminate micro-actions and human attention. In this regard, a human motion capture setup that combines various systems such as joint body, finger, and eye trackers is proposed in combination with a methodology of identifying the intention of human operators as well as for predicting sequences of activities. The approach may lead to safer human-robot collaboration.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">manns_identifying_2021</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{54th {CIRP} {CMS} 2021 - {Towards} {Digitalized} {Manufacturing} 4.0}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Identifying human intention during assembly operations using wearable motion capturing systems including eye focus}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{104}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2212-8271}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S2212827121010532}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.procir.2021.11.155}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2021-11-29}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Procedia CIRP}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Manns, Martin and Tuli, Tadele Belay and Schreiber, Florian}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{human-robot collaboration, Operator 4.0, Eye tracker, human motion capture}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{924--929}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CIRP CMS}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IEEE ETFA</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/mosim.gif">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_knowledge-based_2021" class="col-sm-8">
    
      <div class="title">Knowledge-Based Digital Twin for Predicting Interactions in Human-Robot Collaboration</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kohl, Linus,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Chala, Sisay Adugna,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Ansari, Fazel
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )</em>
      
      
        Sep
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1109/ETFA45728.2021.9613342" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Semantic representation of motions in a human-robot collaborative environment is essential for agile design and development of digital twins (DT) towards ensuring efficient collaboration between humans and robots in hybrid work systems, e.g., in assembly operations. Dividing activities into actions helps to further conceptualize motion models for predicting what a human intends to do in a hybrid work system. However, it is not straightforward to identify human intentions in collaborative operations for robots to understand and collaborate. This paper presents a concept for semantic representation of human actions and intention prediction using a flexible task ontology interface in the semantic data hub stored in a domain knowledge base. This semantic data hub enables the construction of a DT with corresponding reasoning and simulation algorithms. Furthermore, a knowledge-based DT concept is used to analyze and verify the presented use-case of Human-Robot Collaboration in assembly operations. The preliminary evaluation showed a promising reduction of time for assembly tasks, which identifies the potential to i) improve efficiency reflected by reducing costs and errors and ultimately ii) assist human workers in improving decision making. Thus the contribution of the current work involves a marriage of machine learning, robotics, and ontology engineering into DT to improve human-robot interaction and productivity in a collaborative production environment.</p-->
	  <p style="color:Gray;">Semantic representation of motions in a human-robot collaborative environment is essential for agile design and development of digital twins (DT) towards ensuring efficient collaboration between humans and robots in hybrid work systems, e.g., in assembly operations. Dividing activities into actions helps to further conceptualize motion models for predicting what a human intends to do in a hybrid work system. However, it is not straightforward to identify human intentions in collaborative operations for robots to understand and collaborate. This paper presents a concept for semantic representation of human actions and intention prediction using a flexible task ontology interface in the semantic data hub stored in a domain knowledge base. This semantic data hub enables the construction of a DT with corresponding reasoning and simulation algorithms. Furthermore, a knowledge-based DT concept is used to analyze and verify the presented use-case of Human-Robot Collaboration in assembly operations. The preliminary evaluation showed a promising reduction of time for assembly tasks, which identifies the potential to i) improve efficiency reflected by reducing costs and errors and ultimately ii) assist human workers in improving decision making. Thus the contribution of the current work involves a marriage of machine learning, robotics, and ontology engineering into DT to improve human-robot interaction and productivity in a collaborative production environment.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tuli_knowledge-based_2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Knowledge-{Based} {Digital} {Twin} for {Predicting} {Interactions} in {Human}-{Robot} {Collaboration}}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ETFA45728.2021.9613342}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 26th {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA} )}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Kohl, Linus and Chala, Sisay Adugna and Manns, Martin and Ansari, Fazel}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Collaboration, digital twin, Digital twin, human action models, human-robot interaction, Knowledge based systems, machine learning, Ontologies, ontology, Predictive models, Productivity, Semantics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--8}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IEEE ETFA}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{2021130209.pdf}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">wtOnline</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/researching.svg">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="jonek_virtuelle_2021" class="col-sm-8">
    
      <div class="title">Virtuelle Montageplanung mit Motion Capture Systemen/Virtual assembly planning with motion capture systems</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Jonek, Michael,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em>Tuli, Tadele Belay</em>
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>wt Werkstattstechnik online</em>
      
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.37544/1436-4980-2021-04-78" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>In der Planung von teilautomatisierten Montageprozessen ist ein wichtiges Ziel, nicht wertschöpfende Tätigkeiten wie Laufbewegungen zu vermeiden. Studien haben gezeigt, dass die tatsächlichen Laufbewegungen in Montageprozessen von den geplanten Bewegungen abweichen. Dieser Beitrag stellt eine Methode vor, tatsächliche Laufbewegungen mit Motion Capture zu erfassen und in die Laufwegsplanung einzubeziehen, sodass sich Prozess- und Arbeitsplatzgestaltung bereits frühzeitig optimieren lassen.
            &nbsp;
            In planning of semi-automated assembly processes, an important aspect is to avoid non-value-adding activities such as walking movements. Studies have shown that the actual walking movements in assembly processes differ from the planned movements. This paper presents a method of capturing actual walking movements with motion capture and integrating them into walking path planning so that process and workplace design can be optimized at an early stage.</p-->
	  <p style="color:Gray;">In der Planung von teilautomatisierten Montageprozessen ist ein wichtiges Ziel, nicht wertschöpfende Tätigkeiten wie Laufbewegungen zu vermeiden. Studien haben gezeigt, dass die tatsächlichen Laufbewegungen in Montageprozessen von den geplanten Bewegungen abweichen. Dieser Beitrag stellt eine Methode vor, tatsächliche Laufbewegungen mit Motion Capture zu erfassen und in die Laufwegsplanung einzubeziehen, sodass sich Prozess- und Arbeitsplatzgestaltung bereits frühzeitig optimieren lassen.
             
            In planning of semi-automated assembly processes, an important aspect is to avoid non-value-adding activities such as walking movements. Studies have shown that the actual walking movements in assembly processes differ from the planned movements. This paper presents a method of capturing actual walking movements with motion capture and integrating them into walking path planning so that process and workplace design can be optimized at an early stage.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">jonek_virtuelle_2021</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{wtOnline}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Virtuelle {Montageplanung} mit {Motion} {Capture} {Systemen}/{Virtual} assembly planning with motion capture systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{111}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1436-4980}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://elibrary.vdi-verlag.de/index.php?doi=10.37544/1436-4980-2021-04-78}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.37544/1436-4980-2021-04-78}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{04}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2021-05-21}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{wt Werkstattstechnik online}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jonek, Michael and Manns, Martin and Tuli, Tadele Belay}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{256--259}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CIRP CMS</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/dmu.gif">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_path_2021" class="col-sm-8">
    
      <div class="title">Path planning for simulating human motions in manual assembly operations</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zöller, Christian,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Klein, Daniel
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Procedia CIRP</em>
      
      
        Jan
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1016/j.procir.2021.11.156" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Assembly operation simulation can e.g. be used for optimizing complex manual assembly processes in the automotive industry. However, realistic human motion modeling involves difficult tasks such as inserting objects that require constraint definitions and collision detection. In this work, a method that applies the concept of an optimized potential field is employed to generate a motion model unit for simulating human motion for the assembly of pedal cars. This motion model unit is implemented into the newly created MOSIM motion modeling interfaces that is currently being standardized. This approach may allow motion simulation without being tied to a specific 3D environment.</p-->
	  <p style="color:Gray;">Assembly operation simulation can e.g. be used for optimizing complex manual assembly processes in the automotive industry. However, realistic human motion modeling involves difficult tasks such as inserting objects that require constraint definitions and collision detection. In this work, a method that applies the concept of an optimized potential field is employed to generate a motion model unit for simulating human motion for the assembly of pedal cars. This motion model unit is implemented into the newly created MOSIM motion modeling interfaces that is currently being standardized. This approach may allow motion simulation without being tied to a specific 3D environment.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_path_2021</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CIRP CMS}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{54th {CIRP} {CMS} 2021 - {Towards} {Digitalized} {Manufacturing} 4.0}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Path planning for simulating human motions in manual assembly operations}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{104}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2212-8271}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S2212827121010544}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.procir.2021.11.156}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2021-11-29}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Procedia CIRP}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Manns, Martin and Zöller, Christian and Klein, Daniel}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{virtual reality, path planning, MOSIM, human motion simulation, motion model units, pedal cars, potential field}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{930--934}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge"><a href="https://www.springer.com/journal/170" target="_blank" rel="noopener noreferrer">JAMT</a></abbr>
	<!-- img class="col bibone first" src="tadeletuli.com/assets/img/3dprinting.png"-->
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/3dprinting.png">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="frohn-sorensen_3d_2021" class="col-sm-8">
    
      <div class="title">3D printed prototyping tools for flexible sheet metal drawing</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Frohn-Sörensen, Peter,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Geueke, Michael,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kuhnhen, Christopher,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Engel, Bernd
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>The International Journal of Advanced Manufacturing Technology</em>
      
      
        May
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/s00170-021-07312-y" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Due to the change from mass production to mass personalized production and the resulting intrinsic product flexibility, the automotive industry, among others, is looking for cost-efficient and resource-saving production methods to combining global just-in-time production. In addition to geometric manufacturing flexibility, additive manufacturing offers a resource-saving application for rapid prototyping and small series in predevelopment. In this study, the FDM process is utilized to manufacture the tooling to draw a small series of sheet metal parts in combination with the rubber pad forming process. Therefore, a variety of common AM polymer materials (PETG, PLA, and ABS) is compared in compression tests, from which PLA is selected to be applied as sheet metal forming die. For the rubber pad forming process, relevant processing parameters, i.e., press force and rubber cushion hardness, are studied with respect to forming depth. The product batch is examined by optical evaluation using a metrological system. The scans of the tool and sheet metal parts confirm the mechanical integrity of the additively manufactured die from polymer and thus the suitability of this approach for small series in sheet metal drawing processes, e.g., for automotive applications.</p-->
	  <p style="color:Gray;">Due to the change from mass production to mass personalized production and the resulting intrinsic product flexibility, the automotive industry, among others, is looking for cost-efficient and resource-saving production methods to combining global just-in-time production. In addition to geometric manufacturing flexibility, additive manufacturing offers a resource-saving application for rapid prototyping and small series in predevelopment. In this study, the FDM process is utilized to manufacture the tooling to draw a small series of sheet metal parts in combination with the rubber pad forming process. Therefore, a variety of common AM polymer materials (PETG, PLA, and ABS) is compared in compression tests, from which PLA is selected to be applied as sheet metal forming die. For the rubber pad forming process, relevant processing parameters, i.e., press force and rubber cushion hardness, are studied with respect to forming depth. The product batch is examined by optical evaluation using a metrological system. The scans of the tool and sheet metal parts confirm the mechanical integrity of the additively manufactured die from polymer and thus the suitability of this approach for small series in sheet metal drawing processes, e.g., for automotive applications.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">frohn-sorensen_3d_2021</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{JAMT}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{3D} printed prototyping tools for flexible sheet metal drawing}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1433-3015}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/s00170-021-07312-y}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s00170-021-07312-y}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2021-05-31}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The International Journal of Advanced Manufacturing Technology}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Frohn-Sörensen, Peter and Geueke, Michael and Tuli, Tadele Belay and Kuhnhen, Christopher and Manns, Martin and Engel, Bernd}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge"><a href="https://www.springer.com/journal/12369" target="_blank" rel="noopener noreferrer">SORO</a></abbr>
	<!-- img class="col bibone first" src="tadeletuli.com/assets/img/motion_interaction.png"-->
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/motion_interaction.png">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_telepresence_2020" class="col-sm-8">
    
      <div class="title">Telepresence Mobile Robots Design and Control for Social Interaction</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Terefe, Tesfaye Olana,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Rashid, Md Mamun Ur
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Journal of Social Robotics</em>
      
      
        Jul
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/s12369-020-00676-3" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Human–robot interaction has extended its application horizon to simplify how human beings interact with each other through a remotely controlled telepresence robot. The fast growth of communication technologies such as 4G and 5G has elevated the potential to establish stable audio–video-data transmission. However, human–robot physical interactions are still challenging regarding maneuverability, controllability, stability, drive layout, and autonomy. Hence, this paper presents a systematic design and control approach based on the customer’s needs and expectations of telepresence mobile robots for social interactions. A system model and controller design are developed using the Lagrangian method and linear quadratic regulator, respectively, for different scenarios such as flat surface, inclined surface, and yaw (steering). The robot system is capable of traveling uphill (30\{^{}circ }\\∘) and has a variable height (600–1200 mm). The robot is advantageous in developing countries to fill the skill gaps as well as for sharing knowledge and expertise using a virtual and mobile physical presence.</p-->
	  <p style="color:Gray;">Human–robot interaction has extended its application horizon to simplify how human beings interact with each other through a remotely controlled telepresence robot. The fast growth of communication technologies such as 4G and 5G has elevated the potential to establish stable audio–video-data transmission. However, human–robot physical interactions are still challenging regarding maneuverability, controllability, stability, drive layout, and autonomy. Hence, this paper presents a systematic design and control approach based on the customer’s needs and expectations of telepresence mobile robots for social interactions. A system model and controller design are developed using the Lagrangian method and linear quadratic regulator, respectively, for different scenarios such as flat surface, inclined surface, and yaw (steering). The robot system is capable of traveling uphill (30\{^{}circ }\\∘) and has a variable height (600–1200 mm). The robot is advantageous in developing countries to fill the skill gaps as well as for sharing knowledge and expertise using a virtual and mobile physical presence.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_telepresence_2020</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{SORO}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Telepresence {Mobile} {Robots} {Design} and {Control} for {Social} {Interaction}}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1875-4805}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1007/s12369-020-00676-3}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s12369-020-00676-3}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2020-09-14}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Social Robotics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Terefe, Tesfaye Olana and Rashid, Md Mamun Ur}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">VDI-Z</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/researching.svg">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="engel_roboterunterstutztes_2020" class="col-sm-8">
    
      <div class="title">Roboterunterstütztes Biegen von Verbundrohren</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Engel, Bernd,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Manns, Martin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Reuter, Jonas
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>VDI-Z</em>
      
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.37544/0042-1766-2020-12-49" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Das Biegen eines endlosfaserverstärkten, thermoplastischen
Verbundrohres mit Roboterunterstützung könnte die Effizienz des
Biegeprozesses und die Qualität des Endprodukts verbessern.
Nachfolgend wird die Kooperation von Robotern und Biegemaschinen beim Bearbeiten dieser Art von Rohren beschrieben.</p-->
	  <p style="color:Gray;">Das Biegen eines endlosfaserverstärkten, thermoplastischen
Verbundrohres mit Roboterunterstützung könnte die Effizienz des
Biegeprozesses und die Qualität des Endprodukts verbessern.
Nachfolgend wird die Kooperation von Robotern und Biegemaschinen beim Bearbeiten dieser Art von Rohren beschrieben.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">engel_roboterunterstutztes_2020</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{VDI-Z}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Roboterunterstütztes {Biegen} von {Verbundrohren}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{162}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0042-1766}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.37544/0042-1766-2020-12-49}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.37544/0042-1766-2020-12-49}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{de}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2021-03-03}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{VDI-Z}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Engel, Bernd and Manns, Martin and Tuli, Tadele Belay and Reuter, Jonas}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{49--51}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography">
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ECSA</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/researching.svg">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_real-time_2019" class="col-sm-8">
    
      <div class="title">Real-Time Motion Tracking for Humans and Robots in a Collaborative Assembly Task</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings</em>
      
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.3390/ecsa-6-06636" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Human-robot collaboration combines the extended capabilities of humans and robots to create a more inclusive and human-centered production system in the future. However, human safety is the primary concern for manufacturing industries. Therefore, real-time motion tracking is necessary to identify if the human worker body parts enter the restricted working space solely dedicated to the robot. Tracking these motions using decentralized and different tracking systems requires a generic model controller and consistent motion exchanging formats. In this work, our task is to investigate a concept for a unified real-time motion tracking for human-robot collaboration. In this regard, a low cost and game-based motion tracking system, e.g., HTC Vive, is utilized to capture human motion by mapping into a digital human model in the Unity3D environment. In this context, the human model is described using a biomechanical model that comprises joint segments defined by position and orientation. Concerning robot motion tracking, a unified robot description format is used to describe the kinematic trees. Finally, a concept of assembly operation that involves snap joining is simulated to analyze the performance of the system in real-time capability. The distribution of joint variables in spatial-space and time-space is analyzed. The results suggest that real-time tracking in human-robot collaborative assembly environments can be considered to maximize the safety of the human worker. However, the accuracy and reliability of the system regarding system disturbances need to be justified.</p-->
	  <p style="color:Gray;">Human-robot collaboration combines the extended capabilities of humans and robots to create a more inclusive and human-centered production system in the future. However, human safety is the primary concern for manufacturing industries. Therefore, real-time motion tracking is necessary to identify if the human worker body parts enter the restricted working space solely dedicated to the robot. Tracking these motions using decentralized and different tracking systems requires a generic model controller and consistent motion exchanging formats. In this work, our task is to investigate a concept for a unified real-time motion tracking for human-robot collaboration. In this regard, a low cost and game-based motion tracking system, e.g., HTC Vive, is utilized to capture human motion by mapping into a digital human model in the Unity3D environment. In this context, the human model is described using a biomechanical model that comprises joint segments defined by position and orientation. Concerning robot motion tracking, a unified robot description format is used to describe the kinematic trees. Finally, a concept of assembly operation that involves snap joining is simulated to analyze the performance of the system in real-time capability. The distribution of joint variables in spatial-space and time-space is analyzed. The results suggest that real-time tracking in human-robot collaborative assembly environments can be considered to maximize the safety of the human worker. However, the accuracy and reliability of the system regarding system disturbances need to be justified.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_real-time_2019</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ECSA}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Real-{Time} {Motion} {Tracking} for {Humans} and {Robots} in a {Collaborative} {Assembly} {Task}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{42}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{http://creativecommons.org/licenses/by/3.0/}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.mdpi.com/2504-3900/42/1/48}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/ecsa-6-06636}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2020-04-21}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Manns, Martin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Number: 1
  Publisher: Multidisciplinary Digital Publishing Institute}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{collaboration, HRC, human, motion tracking, robot}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{48}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CIRP CMS</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/dmu.gif">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_hierarchical_2019" class="col-sm-8">
    
      <div class="title">Hierarchical motion control for real time simulation of industrial robots</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Manns, Martin
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Procedia CIRP</em>
      
      
        Jan
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1016/j.procir.2019.03.181" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Multi axis machine tools implement real time motion control algorithms. Objects including difficult to manufacture and deformable shapes show variable properties when tools apply a pressure on the object surface. Force compliant industrial robots are used to manipulate deformable objects in real time simulation. In this research, a hierarchical method of motion controlling strategy is presented. The proposed approach is described regarding performance characteristics such as accuracy, repeatability, controllability of the motion segmentation and time dynamics. The result shows that a hierarchical control approach can be considered as a potential candidate to manipulate deformable objects where force requirements are critical.</p-->
	  <p style="color:Gray;">Multi axis machine tools implement real time motion control algorithms. Objects including difficult to manufacture and deformable shapes show variable properties when tools apply a pressure on the object surface. Force compliant industrial robots are used to manipulate deformable objects in real time simulation. In this research, a hierarchical method of motion controlling strategy is presented. The proposed approach is described regarding performance characteristics such as accuracy, repeatability, controllability of the motion segmentation and time dynamics. The result shows that a hierarchical control approach can be considered as a potential candidate to manipulate deformable objects where force requirements are critical.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_hierarchical_2019</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CIRP CMS}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{52nd {CIRP} {Conference} on {Manufacturing} {Systems} ({CMS}), {Ljubljana}, {Slovenia}, {June} 12-14, 2019}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hierarchical motion control for real time simulation of industrial robots}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{81}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2212-8271}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://www.sciencedirect.com/science/article/pii/S221282711930486X}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.procir.2019.03.181}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2019-07-30}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Procedia CIRP}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Manns, Martin}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{hierarchical control, industrial robot, motion planning, realtime simulation}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{713--718}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">JMMP</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/researching.svg">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_automated_2019" class="col-sm-8">
    
      <div class="title">Automated Unsupervised 3D Tool-Path Generation Using Stacked 2D Image Processing Technique</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Tuli, Tadele Belay</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Cesarini, Andrea
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Manufacturing and Materials Processing</em>
      
      
        Dec
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.3390/jmmp3040084" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Tool-path, feed-rate, and depth-of-cut of a tool determine the machining time, tool wear, power consumption, and realization costs. Before the commissioning and production, a preliminary phase of failure-mode identification and effect analysis allows for selecting the optimal machining parameters for cutting, which, in turn, reduces machinery faults, production errors and, ultimately, decreases costs. For this, scalable high-precision path generation algorithms requiring a low amount of computation might be advisable. The present work provides such a simplified scalable computationally low-intensive technique for tool-path generation. From a three dimensional (3D) digital model, the presented algorithm extracts multiple two dimensional (2D) layers. Depending on the required resolution, each layer is converted to a spatial image, and an algebraic analytic closed-form solution provides a geometrical tool path in Cartesian coordinates. The produced tool paths are stacked after processing all object layers. Finally, the generated tool path is translated into a machine code using a G-code generator algorithm. The introduced technique was implemented and simulated using MATLAB® pseudocode with a G-code interpreter and a simulator. The results showed that the proposed technique produced an automated unsupervised reliable tool-path-generator algorithm and reduced tool wear and costs, by allowing the selection of the tool depth-of-cut as an input.</p-->
	  <p style="color:Gray;">Tool-path, feed-rate, and depth-of-cut of a tool determine the machining time, tool wear, power consumption, and realization costs. Before the commissioning and production, a preliminary phase of failure-mode identification and effect analysis allows for selecting the optimal machining parameters for cutting, which, in turn, reduces machinery faults, production errors and, ultimately, decreases costs. For this, scalable high-precision path generation algorithms requiring a low amount of computation might be advisable. The present work provides such a simplified scalable computationally low-intensive technique for tool-path generation. From a three dimensional (3D) digital model, the presented algorithm extracts multiple two dimensional (2D) layers. Depending on the required resolution, each layer is converted to a spatial image, and an algebraic analytic closed-form solution provides a geometrical tool path in Cartesian coordinates. The produced tool paths are stacked after processing all object layers. Finally, the generated tool path is translated into a machine code using a G-code generator algorithm. The introduced technique was implemented and simulated using MATLAB® pseudocode with a G-code interpreter and a simulator. The results showed that the proposed technique produced an automated unsupervised reliable tool-path-generator algorithm and reduced tool wear and costs, by allowing the selection of the tool depth-of-cut as an input.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tuli_automated_2019</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{JMMP}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated {Unsupervised} {3D} {Tool}-{Path} {Generation} {Using} {Stacked} {2D} {Image} {Processing} {Technique}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{http://creativecommons.org/licenses/by/3.0/}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.mdpi.com/2504-4494/3/4/84}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/jmmp3040084}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2021-02-07}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Manufacturing and Materials Processing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay and Cesarini, Andrea}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Number: 4
  Publisher: Multidisciplinary Digital Publishing Institute}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{CAD/CAM, 3D modeling, G-code, image processing, tool path}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{84}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICAST</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/bond.png">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_mathematical_2018" class="col-sm-8">
    
      <div class="title">Mathematical Modeling and Dynamic Simulation of Gantry Robot Using Bond Graph</div>
      <div class="author">
        
          
          
          
          
          
          
            
              <em>Tuli, Tadele Belay</em>
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Information and Communication Technology for Development for Africa</em>
      
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
      <a href="https://www.doi.org/10.1007/978-3-319-95153-9_22" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">DOI</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>This paper presents an initial mathematical modeling and dynamic simulation of gantry robot for the application of printing circuit on board. The classical modeling methods such as Newton-Euler, Kirchoff’s law and Lagrangian fails to unify both electrical and mechanical system models. Here, bond graph approach with robust trajectory planning which uses a blend of quadratic equations on triangular velocity profile is modeled in order to virtually simulate it. In this paper, the algebric mathematical models are developed using maple software. For the sake of simulation, the model is tested on matlab by integrating robot models which are developed by using Solidwork.</p-->
	  <p style="color:Gray;">This paper presents an initial mathematical modeling and dynamic simulation of gantry robot for the application of printing circuit on board. The classical modeling methods such as Newton-Euler, Kirchoff’s law and Lagrangian fails to unify both electrical and mechanical system models. Here, bond graph approach with robust trajectory planning which uses a blend of quadratic equations on triangular velocity profile is modeled in order to virtually simulate it. In this paper, the algebric mathematical models are developed using maple software. For the sake of simulation, the model is tested on matlab by integrating robot models which are developed by using Solidwork.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tuli_mathematical_2018</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICAST}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Lecture {Notes} of the {Institute} for {Computer} {Sciences}, {Social} {Informatics} and {Telecommunications} {Engineering}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mathematical {Modeling} and {Dynamic} {Simulation} of {Gantry} {Robot} {Using} {Bond} {Graph}}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-319-95153-9}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-319-95153-9_22}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Information and {Communication} {Technology} for {Development} for {Africa}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer International Publishing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Mekuria, Fisseha and Nigussie, Ethiopia Enideg and Dargie, Waltenegus and Edward, Mutafugwa and Tegegne, Tesfa}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Robot, Bond graph, Dynamic simulation, Trajectory planning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{228--237}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">M.Sc. Thesis</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/researching.svg">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tulimsc2" class="col-sm-8">
    
      <span id="tulimsc2">Tuli, T. B. (2015). <i>Task and path planning of industrial manipulator robot</i>. http://www5.unitn.it/Biblioteca/en/Web/TesiDocente/193537</span>
    

    <div class="links">
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@thesis</span><span class="p">{</span><span class="nl">tulimsc2</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{M.Sc. Thesis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Task and path planning of industrial manipulator robot}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://www5.unitn.it/Biblioteca/en/Web/TesiDocente/193537}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2012</h2>
  <ol class="bibliography"><li>
<style>
  .abbr .badge,
  .award .badge {
    max-width: 120px; /* Adjust the value to your preferred width */
    overflow: hidden;
    text-overflow: ellipsis; /* Adds an ellipsis (...) for overflow text */
  }
</style>


<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">M.Sc. Thesis</abbr>
    
  
  

 
  <!--
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="tadeletuli.com/assets/img/researching.svg">
    </div>
  </div> -->

  
  
  </div>
  
  
  <div id="tuli_finite_2012" class="col-sm-8">
    
      <div class="title">Finite Element Analysis of Bus Body Structures: Case study at Bishoftu Automotive and Locomotive Industry, Ethiopia Adama, Adama Science and Technology University 2012</div>
      <div class="author">
        
          
          
          
          
          
          
            
              <em>Tuli, Tadele Belay</em>
            
          
        
      </div>

      <div class="periodical">
      
      
        Oct
      
      
        2012
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <!--p>Most of Automotive and Locomotive Industries lack the basic scientific methods like computer application to design and manufacture vehicle body structures in order to analyze static and dynamic loads effect. This work presents a finite element model of a city bus for both static and dynamic condition. Their effects are verified by using quarter car and half car multibody models. The basic finite element models are represented by a computer program with a graphic user interface using matlab environment. Designers and manufacturer’s can observe the responses of these models on different condition. This work, therefore introduces the mathematical models using a concept of finite element method to introduce how advanced modeling and analysis tools can be used for design and manufacturing of city bus body structures. Both vehicle industries and academic sectors can use this work as a further reference.</p-->
	  <p style="color:Gray;">Most of Automotive and Locomotive Industries lack the basic scientific methods like computer application to design and manufacture vehicle body structures in order to analyze static and dynamic loads effect. This work presents a finite element model of a city bus for both static and dynamic condition. Their effects are verified by using quarter car and half car multibody models. The basic finite element models are represented by a computer program with a graphic user interface using matlab environment. Designers and manufacturer’s can observe the responses of these models on different condition. This work, therefore introduces the mathematical models using a concept of finite element method to introduce how advanced modeling and analysis tools can be used for design and manufacturing of city bus body structures. Both vehicle industries and academic sectors can use this work as a further reference.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@book</span><span class="p">{</span><span class="nl">tuli_finite_2012</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{M.Sc. Thesis}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Finite {Element} {Analysis} of {Bus} {Body} {Structures}: {Case} study at {Bishoftu} {Automotive} and {Locomotive} {Industry}, {Ethiopia} {Adama}, {Adama} {Science} and {Technology} {University} 2012}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-659-25932-6}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Finite {Element} {Analysis} of {Bus} {Body} {Structures}}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{English}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{LAP LAMBERT Academic Publishing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tuli, Tadele Belay}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2024 Tadele B. Tuli.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
    Last updated: January 04, 2024.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
